---
title: 特征选择
tags:
- ML
- feature engineer
- feature select
categories:
- Data Mining
- feature select
---
# 特征选择

当面临数据特征维度较高的情况， 特征选择是除降维算法外的另一种手段。通过选择有价值的特征，可以降低计算量，同时排除冗余特征的干扰。

## 特征选择要素
那么如何做特征选择了？这个问题可以拆分为两个更具体的问题，那就是如何做**特征搜索**以及如何做**特征评价**

### 特征搜索

假设有N个特征，这N个特征可能的子集个数为$2^N$，显然，采用遍历的方式搜索特征，时间开销太大。因此，往往采用**贪心**的方式去搜索表现良好的特征。得到的可能往往只是局部最优而非全局最优的特征子集（在选定某种特征评价方式的情况下）。

### 特征评价

对于选定的两个特征子集D1和D2, 通过什么手段来评估这两个子集的优劣即为特征评价。


## 特征选择方法
特征选择的过程就是对特征搜索和特征评价综合运用的过程。按照机器学习流程中特征选择和模型训练的不同组合策略，可将特征选择方法大致归纳为为三类：过滤式 包裹式 嵌入式。

### 过滤式

先进行特征选择，然后进行模型训练。

过滤式特征选择的特点在于，不需要引入额外的模型， 考察特征与目标的相关性， 相关性选择相关性高的特征。

大致过程可描述为：使用一定策略，评估特征$f_i$和label之间的相关度，然后设定阈值，选择出相关性较高的特征。

常用的评估方法一般有一下几种：

#### 方差法

考察特征列自身的波动情况，去除波动较小的特征。
计算各特征列的方差，与阈值比较，选择方差大于阈值的列。
方差公式 $$Var(X) = \sum_{i=1}^n\frac{(x_i -\bar{x})^2 }{n}$$‘

#### 卡方检验法

//todo

#### 互信息法

// todo

#### Relif



